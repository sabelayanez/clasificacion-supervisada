{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys \n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "\n",
    "from constants import excel_filename, dataset_name, CV\n",
    "from modelos.RegresionLogistica import regresion_logistica\n",
    "from modelos.CNN import cnn1, cnn2\n",
    "from modelos.KNN import knn\n",
    "from modelos.ArbolDeDecision import arbol_decision\n",
    "\n",
    "## guardar datos en excel ##\n",
    "def save_to_excel(datos):\n",
    "    # Si el archivo ya existe, se leerá y se agregará nueva información\n",
    "    metodo =  datos.pop(\"Método\")\n",
    "    if os.path.exists(excel_filename):\n",
    "        results = pd.read_excel(excel_filename, index_col=0)\n",
    "        df = pd.DataFrame(datos, index=metodo)\n",
    "        results = pd.concat([results, df], ignore_index=False)\n",
    "        results.to_excel(excel_filename)\n",
    "    else:\n",
    "        results = pd.DataFrame(datos)\n",
    "        results.index = metodo\n",
    "        results.to_excel(excel_filename)\n",
    "\n",
    "#función para hacer la gráfica después de evaluar el rendimiento\n",
    "def plot_rendimiento(exactitud, sensibilidad, precision, matriz_confusion, clases, fpr_micro, tpr_micro, roc_auc_micro, fpr, tpr, roc_auc):\n",
    "    # Imprimir métricas de evaluación\n",
    "    print(\"Exactitud    : %.2f %%\" % exactitud)\n",
    "    print(\"Sensibilidad : %.2f %%\" % sensibilidad)\n",
    "    print(\"Precisión    : %.2f %%\" % precision)\n",
    "\n",
    "    #matriz de confusion\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion, display_labels=clases)\n",
    "    disp.plot()\n",
    "    disp.figure_.suptitle(\"Matriz de confusión\")\n",
    "    disp.figure_.set_dpi(100)\n",
    "    plt.xlabel(\"Clase predicha\")\n",
    "    plt.ylabel(\"Clase real\")\n",
    "    plt.show()\n",
    "\n",
    "    #roc y auc\n",
    "    plt.figure()\n",
    "    plt.plot(fpr_micro, tpr_micro, color='red', lw=2, label='Curva ROC micro-average (AUC = %0.3f)' % roc_auc_micro)\n",
    "    plt.plot([0, 1], [0, 1], color='k', lw=1, linestyle='--')\n",
    "    plt.xlabel('Tasa de Falsos Positivos')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Curvas ROC por clase\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['aqua', 'blue', 'violet', 'gold', 'orange', 'pink', 'tan', 'purple', 'lime', 'red']\n",
    "    for i in range(len(clases)):\n",
    "        plt.plot(fpr[i], tpr[i], color=colors[i], lw=1, label='ROC clase %i (area = %0.3f)' % (i, roc_auc[i]))\n",
    "\n",
    "    plt.plot(fpr_micro, tpr_micro, color='red', lw=2, linestyle=':', label='Curva ROC micro-average (AUC = %0.3f)' % roc_auc_micro)\n",
    "    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
    "    plt.xlabel('Tasa de Falsos Positivos')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "    plt.title('Curva ROC por clase')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "## evaluar rendimiento ##\n",
    "def evaluar_rendimiento(model, X_test, y_test, nombre_metodo, pca):\n",
    "    # Aplanar las imágenes de prueba\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)  # Aplanar a 2D: [n_samples, n_features]\n",
    "    \n",
    "    # Normalizar los datos de prueba (debe coincidir con la normalización de X_train)\n",
    "    scaler = StandardScaler()\n",
    "    X_test_scaled = scaler.fit_transform(X_test_flat)\n",
    "    \n",
    "    # Aplicar PCA al conjunto de prueba (debe ser la misma transformación que en el entrenamiento)\n",
    "    if pca is not None:\n",
    "        X_test_pca = pca.transform(X_test_scaled)  # Aquí usamos el PCA entrenado para transformar X_test\n",
    "    else:\n",
    "        X_test_pca = model.pca.transform(X_test_scaled)  # Aquí usamos el PCA entrenado para transformar X_test\n",
    "\n",
    "    # Hacer predicciones\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "\n",
    "    # Métricas de rendimiento\n",
    "    precision = 100 * accuracy_score(y_test, y_pred)\n",
    "    sensibility = 100 * recall_score(y_test, y_pred, average='macro')\n",
    "    precision_score_value = 100 * precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Informe de clasificación\n",
    "    print(\"Informe de evaluación del clasificador sobre el conjunto de test:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "\n",
    "    # ROC y AUC\n",
    "    n_classes = len(np.unique(y_test))\n",
    "    y_test_bin = label_binarize(y_test, classes=np.arange(0, n_classes, 1))\n",
    "    y_score = model.predict_proba(X_test_pca)\n",
    "\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "\n",
    "    # ROC por clase\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Aquí iría la función para guardar en Excel (asegúrate de que 'Excel_data' y 'save_to_excel' están bien implementadas)\n",
    "    datos = {\n",
    "        \"Método\": [nombre_metodo],\n",
    "        \"Exactitud\": [100 * accuracy_score(y_test, y_pred)],\n",
    "        \"Precisión\": [100 * precision_score(y_test, y_pred, average='macro')],\n",
    "        \"Matriz de confusión\": [cm],\n",
    "        \"fpr_micro\": [fpr_micro],\n",
    "        \"tpr_micro\": [tpr_micro],\n",
    "        \"roc_auc_micro\": [roc_auc_micro],\n",
    "        \"fpr\": [fpr],\n",
    "        \"tpr\": [tpr],\n",
    "        \"roc_auc\": [roc_auc]\n",
    "    }\n",
    "    save_to_excel(datos)\n",
    "    # plot rendimiento\n",
    "    plot_rendimiento(\n",
    "        precision,\n",
    "        sensibility,\n",
    "        precision_score_value,\n",
    "        cm,\n",
    "        model.classes_,\n",
    "        fpr_micro,\n",
    "        tpr_micro,\n",
    "        roc_auc_micro,\n",
    "        fpr,\n",
    "        tpr,\n",
    "        roc_auc\n",
    "    )\n",
    "\n",
    "\n",
    "## cargar imágenes ##\n",
    "def cargar_imagenes(image_path, target_size=(256, 256), channel_mode=\"rgb\"):\n",
    "\n",
    "    img_list = []\n",
    "    labels = []\n",
    "    classes = os.listdir(image_path)\n",
    "\n",
    "    for folder in classes:\n",
    "        folder_path = os.path.join(image_path, folder)\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    img = Image.open(os.path.join(folder_path, filename)).convert(\"RGB\")\n",
    "                    img_array = np.array(img)\n",
    "\n",
    "                    # Redimensionar\n",
    "                    img_resized = resize(img_array, target_size, anti_aliasing=True)\n",
    "\n",
    "                    # Modos de canal\n",
    "                    if channel_mode == \"grayscale\":\n",
    "                        img_resized = rgb2gray(img_resized)  # Convertir a escala de grises\n",
    "                    elif channel_mode == \"r\":  # Canal rojo\n",
    "                        img_resized = img_resized[:, :, 0]\n",
    "                    elif channel_mode == \"g\":  # Canal verde\n",
    "                        img_resized = img_resized[:, :, 1]\n",
    "                    elif channel_mode == \"b\":  # Canal azul\n",
    "                        img_resized = img_resized[:, :, 2]\n",
    "                    elif channel_mode == \"rgb\":\n",
    "                        img_resized = (img_resized * 255).astype(np.uint8)  # Restaurar valores de píxeles\n",
    "\n",
    "                    img_resized = img_resized / 255.0  # Normalizar\n",
    "                    img_list.append(img_resized)\n",
    "                    labels.append(folder)  # Guardar etiqueta\n",
    "\n",
    "    return np.array(img_list), np.array(labels)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(dataset_name)\n",
    "\n",
    "# cargar train y test\n",
    "X_train_rgb, y_train = cargar_imagenes(os.path.join(path, 'flowers/flowers/flower_photos/train'), channel_mode=\"rgb\")\n",
    "X_test_rgb, y_test = cargar_imagenes(os.path.join(path, 'flowers/flowers/flower_photos/test'), channel_mode=\"rgb\")\n",
    "X_train_gray, _ = cargar_imagenes(os.path.join(path, 'flowers/flowers/flower_photos/train'), channel_mode=\"grayscale\")\n",
    "X_test_gray, _ = cargar_imagenes(os.path.join(path, 'flowers/flowers/flower_photos/test'), channel_mode=\"grayscale\")\n",
    "X_train_rgb_64, y_train = cargar_imagenes(os.path.join(path, 'flowers/flowers/flower_photos/train'), target_size=(64, 64))\n",
    "X_test_rgb_64, y_test = cargar_imagenes(os.path.join(path, 'flowers/flowers/flower_photos/test'), target_size=(64, 64))\n",
    "\n",
    "# Codificar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train_rgb_64.reshape(X_train_rgb_64.shape[0], -1)  # Convierte a 2D (n_samples, n_features)\n",
    "X_test_flattened = X_test_rgb_64.reshape(X_test_rgb_64.shape[0], -1)      # Hace lo mismo para el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "CV = 5\n",
    "scoring = ['precision_macro', 'recall_macro', 'precision_micro', 'recall_micro', 'f1_macro', 'accuracy', 'roc_auc_ovo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, datasets, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "def cnn1():\n",
    "    model = models.Sequential()\n",
    "    model.add(keras.Input(shape=(64, 64, 3)))  # Nueva dimensión de entrada\n",
    "\n",
    "    # Primera capa convolucional\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Segunda capa convolucional\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Global Average Pooling para reducir dimensionalidad\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Capa densa\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))  # Dropout para evitar el sobreajuste\n",
    "\n",
    "    # Capa de salida con 5 clases\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compilación del modelo\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def cnn2():\n",
    "    model = models.Sequential()\n",
    "    model.add(keras.Input(shape=(64, 64, 3)))  # Nueva dimensión de entrada\n",
    "\n",
    "    # Bloque 1\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Bloque 2\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Global Average Pooling en lugar de Flatten\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Capa densa\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Capa de salida con Softmax\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compilación del modelo\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.2426 - loss: 1.6007 - val_accuracy: 0.3347 - val_loss: 1.5547\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - accuracy: 0.2273 - loss: 1.6003 - val_accuracy: 0.3390 - val_loss: 1.5447\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 146ms/step - accuracy: 0.2594 - loss: 1.5953 - val_accuracy: 0.3206 - val_loss: 1.5498\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 155ms/step - accuracy: 0.2311 - loss: 1.5966 - val_accuracy: 0.3771 - val_loss: 1.5194\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - accuracy: 0.2163 - loss: 1.6054 - val_accuracy: 0.3588 - val_loss: 1.5475\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\n",
      "precision_macro: [0.2847789311681259, 0.2508539155125732, 0.2700928072505196, 0.32312819883765365, 0.36230220428506843]\n",
      "\n",
      "recall_macro: [0.31262344296827055, 0.299148795010864, 0.2712002200976683, 0.3460466712184833, 0.33169610316792825]\n",
      "\n",
      "precision_micro: [0.3347457627118644, 0.3389830508474576, 0.3206214689265537, 0.3771186440677966, 0.3587570621468927]\n",
      "\n",
      "recall_micro: [0.3347457627118644, 0.3389830508474576, 0.3206214689265537, 0.3771186440677966, 0.3587570621468927]\n",
      "\n",
      "f1_macro: [0.2511499450079433, 0.24265712821694282, 0.19527660869452607, 0.2924371127350146, 0.29129874614623874]\n",
      "\n",
      "accuracy: [0.3347457627118644, 0.3389830508474576, 0.3206214689265537, 0.3771186440677966, 0.3587570621468927]\n",
      "\n",
      "roc_auc: [np.float64(0.7409002681110595), np.float64(0.7188553766204164), np.float64(0.707852314616112), np.float64(0.7379699903410017), np.float64(0.7146018413471857)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Definir el número de folds\n",
    "CV = 5\n",
    "kf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Para almacenar las métricas de cada fold\n",
    "scores = {\n",
    "    \"precision_macro\": [],\n",
    "    \"recall_macro\": [],\n",
    "    \"precision_micro\": [],\n",
    "    \"recall_micro\": [],\n",
    "    \"f1_macro\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"roc_auc\": []\n",
    "}\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_rgb_64, y_train_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}/{CV}\")\n",
    "\n",
    "    # Dividir los datos en train y validación\n",
    "    X_train_fold, X_val_fold = X_train_rgb_64[train_index], X_train_rgb_64[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_encoded[train_index], y_train_encoded[val_index]\n",
    "\n",
    "    # Crear un nuevo modelo en cada fold\n",
    "    model = cnn1()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train_fold, y_train_fold,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(X_val_fold, y_val_fold),\n",
    "              verbose=1)\n",
    "\n",
    "    # Predecir en el conjunto de validación\n",
    "    y_pred_probs = model.predict(X_val_fold)  # Probabilidades\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)  # Clases predichas\n",
    "\n",
    "    # Calcular métricas\n",
    "    scores[\"accuracy\"].append(accuracy_score(y_val_fold, y_pred))\n",
    "    scores[\"precision_macro\"].append(precision_score(y_val_fold, y_pred, average=\"macro\"))\n",
    "    scores[\"recall_macro\"].append(recall_score(y_val_fold, y_pred, average=\"macro\"))\n",
    "    scores[\"precision_micro\"].append(precision_score(y_val_fold, y_pred, average=\"micro\"))\n",
    "    scores[\"recall_micro\"].append(recall_score(y_val_fold, y_pred, average=\"micro\"))\n",
    "    scores[\"f1_macro\"].append(f1_score(y_val_fold, y_pred, average=\"macro\"))\n",
    "\n",
    "    # Calcular ROC AUC (solo si es clasificación multiclase con one-hot encoding)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_pred_probs, multi_class=\"ovr\")\n",
    "        scores[\"roc_auc\"].append(roc_auc)\n",
    "    except ValueError:\n",
    "        scores[\"roc_auc\"].append(None)\n",
    "\n",
    "# Mostrar los resultados de cada fold\n",
    "for metric, values in scores.items():\n",
    "    print(f\"\\n{metric}: {values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 659ms/step - accuracy: 0.3524 - loss: 1.5764 - val_accuracy: 0.1992 - val_loss: 1.6044\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 736ms/step - accuracy: 0.3943 - loss: 1.5934 - val_accuracy: 0.2528 - val_loss: 1.6404\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 802ms/step - accuracy: 0.3700 - loss: 1.5561 - val_accuracy: 0.3390 - val_loss: 1.4927\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step\n",
      "\n",
      "Fold 4/5\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 715ms/step - accuracy: 0.3688 - loss: 1.5369 - val_accuracy: 0.3588 - val_loss: 1.5447\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 679ms/step - accuracy: 0.3668 - loss: 1.6592 - val_accuracy: 0.2754 - val_loss: 1.5369\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "\n",
      "precision_macro: [0.2847789311681259, 0.2508539155125732, 0.2700928072505196, 0.32312819883765365, 0.36230220428506843, 0.3395939234589541, 0.20221967963386728, 0.4634548216270704, 0.368373134569026, 0.5318170924785609]\n",
      "\n",
      "recall_macro: [0.31262344296827055, 0.299148795010864, 0.2712002200976683, 0.3460466712184833, 0.33169610316792825, 0.22108467468695786, 0.28808384692100175, 0.3679032430397086, 0.3883558084258352, 0.307764202521521]\n",
      "\n",
      "precision_micro: [0.3347457627118644, 0.3389830508474576, 0.3206214689265537, 0.3771186440677966, 0.3587570621468927, 0.19915254237288135, 0.2528248587570621, 0.3389830508474576, 0.3587570621468927, 0.2754237288135593]\n",
      "\n",
      "recall_micro: [0.3347457627118644, 0.3389830508474576, 0.3206214689265537, 0.3771186440677966, 0.3587570621468927, 0.19915254237288135, 0.2528248587570621, 0.3389830508474576, 0.3587570621468927, 0.2754237288135593]\n",
      "\n",
      "f1_macro: [0.2511499450079433, 0.24265712821694282, 0.19527660869452607, 0.2924371127350146, 0.29129874614623874, 0.1218560222056079, 0.16294159068339803, 0.29146795906503564, 0.30428625493218975, 0.23425678317303927]\n",
      "\n",
      "accuracy: [0.3347457627118644, 0.3389830508474576, 0.3206214689265537, 0.3771186440677966, 0.3587570621468927, 0.19915254237288135, 0.2528248587570621, 0.3389830508474576, 0.3587570621468927, 0.2754237288135593]\n",
      "\n",
      "roc_auc: [np.float64(0.7409002681110595), np.float64(0.7188553766204164), np.float64(0.707852314616112), np.float64(0.7379699903410017), np.float64(0.7146018413471857), np.float64(0.6628040658904858), np.float64(0.6809515580083626), np.float64(0.7674006212819424), np.float64(0.7241537084399328), np.float64(0.750165809610486)]\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_rgb_64, y_train_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}/{CV}\")\n",
    "\n",
    "    # Dividir los datos en train y validación\n",
    "    X_train_fold, X_val_fold = X_train_rgb_64[train_index], X_train_rgb_64[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_encoded[train_index], y_train_encoded[val_index]\n",
    "\n",
    "    # Crear un nuevo modelo en cada fold\n",
    "    model2 = cnn2()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model2.fit(X_train_fold, y_train_fold,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(X_val_fold, y_val_fold),\n",
    "              verbose=1)\n",
    "\n",
    "    # Predecir en el conjunto de validación\n",
    "    y_pred_probs = model2.predict(X_val_fold)  # Probabilidades\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)  # Clases predichas\n",
    "\n",
    "    # Calcular métricas\n",
    "    scores[\"accuracy\"].append(accuracy_score(y_val_fold, y_pred))\n",
    "    scores[\"precision_macro\"].append(precision_score(y_val_fold, y_pred, average=\"macro\"))\n",
    "    scores[\"recall_macro\"].append(recall_score(y_val_fold, y_pred, average=\"macro\"))\n",
    "    scores[\"precision_micro\"].append(precision_score(y_val_fold, y_pred, average=\"micro\"))\n",
    "    scores[\"recall_micro\"].append(recall_score(y_val_fold, y_pred, average=\"micro\"))\n",
    "    scores[\"f1_macro\"].append(f1_score(y_val_fold, y_pred, average=\"macro\"))\n",
    "\n",
    "    # Calcular ROC AUC (solo si es clasificación multiclase con one-hot encoding)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_pred_probs, multi_class=\"ovr\")\n",
    "        scores[\"roc_auc\"].append(roc_auc)\n",
    "    except ValueError:\n",
    "        scores[\"roc_auc\"].append(None)\n",
    "\n",
    "# Mostrar los resultados de cada fold\n",
    "for metric, values in scores.items():\n",
    "    print(f\"\\n{metric}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "def arbol_decision_vgg16(X_train, y_train, X_test, input_shape=(256,256,3)):\n",
    "    # Cargar VGG16 preentrenado SIN la capa superior\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "    # Extraer características con VGG16\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "    # Aplanar las características extraídas\n",
    "    X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)\n",
    "    X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)\n",
    "\n",
    "    # Validación cruzada con las características extraídas\n",
    "    model_tree_vgg = DecisionTreeClassifier(criterion=\"gini\", max_depth=10, random_state=42)\n",
    "\n",
    "    scores_tree_vgg = cross_validate(model_tree_vgg, X_train_features_flat, y_train, cv=5, scoring=scoring)\n",
    "    #print(f\"Precisión media de validación cruzada con VGG16: {np.mean(scores_tree_vgg):.4f}\")\n",
    "    # Mostrar los resultados promedio de cada métrica\n",
    "    print(\"Resultados de Validación Cruzada:\")\n",
    "    for metric in scoring:\n",
    "        mean_score = np.mean(scores_tree_vgg[f'test_{metric}'])\n",
    "        print(f\"{metric}: {mean_score:.4f}\")\n",
    "    # Entrenar el modelo con todo el conjunto de entrenamiento\n",
    "    model_tree_vgg.fit(X_train_features_flat, y_train)\n",
    "\n",
    "    return model_tree_vgg, scores_tree_vgg\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "def arbol_vgg16_pca(X_train, y_train, X_test, input_shape=(256,256,3), n_components=500):\n",
    "    # Cargar VGG16 preentrenado SIN la capa superior\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "    # Extraer características con VGG16\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "    # Aplanar las características extraídas\n",
    "    X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)\n",
    "    X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)\n",
    "\n",
    "    # Reducción de dimensionalidad con PCA\n",
    "    pca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "    X_train_pca = pca.fit_transform(X_train_features_flat)\n",
    "    X_test_pca = pca.transform(X_test_features_flat)\n",
    "\n",
    "    # Validación cruzada con PCA y VGG16\n",
    "    model_tree_pca = DecisionTreeClassifier(criterion=\"gini\", max_depth=20, random_state=42)\n",
    "    \n",
    "    scores_tree_pca = cross_validate(model_tree_pca, X_train_pca, y_train, cv=5, scoring=scoring)\n",
    "    #print(f\"Precisión media de validación cruzada con VGG16 y PCA: {np.mean(scores_tree_pca):.4f}\")\n",
    "    print(\"Resultados de Validación Cruzada:\")\n",
    "    for metric in scoring:\n",
    "        mean_score = np.mean(scores_tree_pca[f'test_{metric}'])\n",
    "        print(f\"{metric}: {mean_score:.4f}\")\n",
    "    # Entrenar el modelo con todo el conjunto de entrenamiento\n",
    "    model_tree_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "    return model_tree_pca, scores_tree_pca\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 7s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step\n",
      "Resultados de Validación Cruzada:\n",
      "precision_macro: 0.4695\n",
      "recall_macro: 0.4642\n",
      "precision_micro: 0.4737\n",
      "recall_micro: 0.4737\n",
      "f1_macro: 0.4643\n",
      "accuracy: 0.4737\n",
      "roc_auc_ovo: 0.6958\n"
     ]
    }
   ],
   "source": [
    "model_tree_vgg = arbol_decision_vgg16(X_train_rgb, y_train_encoded, X_test_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m716s\u001b[0m 6s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4s/step\n",
      "Resultados de Validación Cruzada:\n",
      "precision_macro: 0.5037\n",
      "recall_macro: 0.5047\n",
      "precision_micro: 0.5147\n",
      "recall_micro: 0.5147\n",
      "f1_macro: 0.5035\n",
      "accuracy: 0.5147\n",
      "roc_auc_ovo: 0.6883\n"
     ]
    }
   ],
   "source": [
    "model_tree_pca = arbol_vgg16_pca(X_train_rgb, y_train, X_test_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def random_forest(X_train, y_train, X_test, input_shape=(256, 256, 3)):\n",
    "    # Crear y entrenar el modelo Random Forest\n",
    "    \n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    scores_rf = cross_validate(rf_model, X_train_flat, y_train, cv=5)\n",
    "    print(f'Accuracy scores for each fold: {scores_rf}')\n",
    "    print(f'Mean cross-validation accuracy: {scores_rf[\"test_score\"].mean()}')\n",
    "\n",
    "    rf_model.fit(X_train_flat, y_train)\n",
    "\n",
    "    # Hacer predicciones\n",
    "    y_pred_rf = rf_model.predict(X_test_flat)\n",
    "\n",
    "    return rf_model\n",
    "\n",
    "def rforest_vgg16_pca(X_train, y_train, X_test, input_shape=(256, 256, 3), n_components=500):\n",
    "    \n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "    # Extraer características con VGG16\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "    # Aplanar las características extraídas\n",
    "    X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)\n",
    "    X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)\n",
    "    \n",
    "    pca = PCA(n_components=500, svd_solver='randomized')  # Elegimos 200 características más relevantes\n",
    "    X_train_pca = pca.fit_transform(X_train_features_flat)\n",
    "    X_test_pca = pca.transform(X_test_features_flat)\n",
    "\n",
    "    model_rf = RandomForestClassifier(\n",
    "    n_estimators=200,  # Más árboles = mejor generalización\n",
    "    max_depth=30,  # Mayor profundidad\n",
    "    min_samples_split=3,  # Menos datos necesarios para dividir\n",
    "    min_samples_leaf=2,  # Evita ramas muy pequeñas\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "    scores_rf_pca = cross_validate(model_rf, X_train_pca, y_train, cv=5)\n",
    "    print(f'Accuracy scores for each fold: {scores_rf_pca}')\n",
    "    #print(f'Mean cross-validation accuracy: {scores_rf_pca.mean()}')\n",
    "    print(f'Mean cross-validation accuracy: {scores_rf_pca[\"test_score\"].mean()}')\n",
    "    model_rf.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Precisión con VGG16 + PCA\n",
    "    return model_rf\n",
    "\n",
    "def extract_hog_features(images):\n",
    "\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        feature = hog(img, pixels_per_cell=(16, 16), cells_per_block=(2, 2), feature_vector=True)\n",
    "        hog_features.append(feature)\n",
    "\n",
    "    return np.array(hog_features)\n",
    "\n",
    "def rforest_vgg16_pca_hog(X_train, y_train, X_test, X_train_gray, X_test_gray, input_shape=(256,256,3), n_components=500):\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "    # Extraer características con VGG16\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "    # Aplanar las características extraídas\n",
    "    X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)\n",
    "    X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)\n",
    "    \n",
    "    pca = PCA(n_components=500, svd_solver='randomized')  # Elegimos 200 características más relevantes\n",
    "    X_train_pca = pca.fit_transform(X_train_features_flat)\n",
    "    X_test_pca = pca.transform(X_test_features_flat)\n",
    "\n",
    "    # Extraer características HOG\n",
    "    X_train_hog = extract_hog_features(X_train_gray)\n",
    "    X_test_hog = extract_hog_features(X_test_gray)\n",
    "    # Normalizar características\n",
    "    scaler = StandardScaler()\n",
    "    X_train_hog_scaled = scaler.fit_transform(X_train_hog)\n",
    "    X_test_hog_scaled = scaler.transform(X_test_hog)\n",
    "\n",
    "    # Concatenar VGG16 + HOG\n",
    "    X_train_combined = np.hstack((X_train_pca, X_train_hog_scaled))\n",
    "    X_test_combined = np.hstack((X_test_pca, X_test_hog_scaled))\n",
    "\n",
    "    # Entrenar Random Forest con características combinadas\n",
    "    model_rf_combined = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)\n",
    "    \n",
    "    cv_scores = cross_validate(model_rf_combined, X_train_combined, y_train, cv=5)\n",
    "    print(f'Accuracy scores for each fold: {cv_scores}')\n",
    "    #print(f'Mean cross-validation accuracy: {cv_scores.mean()}')\n",
    "    print(f'Mean cross-validation accuracy: {cv_scores[\"test_score\"].mean()}')\n",
    "\n",
    "    model_rf_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "    return model_rf_combined, cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each fold: {'fit_time': array([25.2925384 , 24.76381326, 24.87243223, 25.19280028, 25.19163847]), 'score_time': array([0.05431128, 0.0724647 , 0.0597024 , 0.05120516, 0.05671883]), 'test_score': array([0.52824859, 0.52118644, 0.53107345, 0.49576271, 0.52966102])}\n",
      "Mean cross-validation accuracy: 0.5211864406779662\n"
     ]
    }
   ],
   "source": [
    "rf_model = random_forest(X_train_rgb_64, y_train, X_test_rgb_64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 6s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3s/step\n",
      "Accuracy scores for each fold: {'fit_time': array([14.03428292, 13.8959868 , 13.93460393, 14.09746408, 13.97438383]), 'score_time': array([0.05023885, 0.04000211, 0.03836036, 0.03998375, 0.03776574]), 'test_score': array([0.67937853, 0.72175141, 0.72316384, 0.69915254, 0.69774011])}\n",
      "Mean cross-validation accuracy: 0.7042372881355933\n"
     ]
    }
   ],
   "source": [
    "rf_model_vgg16_pca = rforest_vgg16_pca(X_train_rgb, y_train, X_test_rgb, input_shape=(256, 256, 3), n_components=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_model_vgg16_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrf_model_vgg16_pca\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_model_vgg16_pca' is not defined"
     ]
    }
   ],
   "source": [
    "rf_model_vgg16_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 6s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3s/step\n",
      "Accuracy scores for each fold: {'fit_time': array([55.87965798, 54.57125759, 55.30457354, 55.33034348, 55.56378818]), 'score_time': array([0.09120703, 0.08135343, 0.08197927, 0.08349848, 0.08234692]), 'test_score': array([0.52118644, 0.52683616, 0.53672316, 0.52542373, 0.53954802])}\n",
      "Mean cross-validation accuracy: 0.5299435028248588\n"
     ]
    }
   ],
   "source": [
    "rf_model_vgg16_pca_hog = rforest_vgg16_pca_hog(X_train_rgb, y_train, X_test_rgb, X_train_gray, X_test_gray, input_shape=(256, 256, 3), n_components=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
